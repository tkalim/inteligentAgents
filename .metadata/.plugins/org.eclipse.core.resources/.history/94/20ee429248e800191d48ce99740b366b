package reactive;

import java.util.HashMap;
import java.util.Map;
import java.util.Random;

import logist.agent.Agent;
import logist.behavior.ReactiveBehavior;
import logist.plan.Action;
import logist.plan.Action.Move;
import logist.plan.Action.Pickup;
import logist.simulation.Vehicle;
import logist.task.Task;
import logist.task.TaskDistribution;
import logist.topology.Topology;
import logist.topology.Topology.City;

public class ReactiveSimulation implements ReactiveBehavior {

	private Random random;
	private double pPickup;
	private int numActions;
	private Agent myAgent;

	@Override
	public void setup(Topology topology, TaskDistribution td, Agent agent) {

		// Reads the discount factor from the agents.xml file.
		// If the property is not present it defaults to 0.95
		Double discount = agent.readProperty("discount-factor", Double.class,
				0.95);

		//get the cost per km
		double costPerKm = agent.vehicles().get(0).costPerKm();

		this.random = new Random();
		this.pPickup = discount;
		this.numActions = 0;
		this.myAgent = agent;


		//Creating the states
		Map<String, State> states_map = new HashMap<String, State>();
		int id = 0;
		for (City from : topology) {
			for(City destination: topology) {
				Task task = new Task(id, from, destination, td.reward(from, destination), td.weight(from, destination));
				id ++;
				State state = new State(from, task);
				states_map.put(state.getState_id(), state);
			}
		}

		//Creating the actions
		Map<Integer, Action_at> actionsMap = new HashMap<Integer, Action_at>();
		int action_id = 0;
		for (City currentCity : topology) {
			Action_at actionAccept = new Action_at(true, currentCity);
			Action_at actionRefuse = new Action_at(false, currentCity);
			actions_map.put(action_id, actionAccept);
			action_id++;
			actions_map.put(action_id, actionRefuse);
		}

		//filling the Reward table
		Map<State, HashMap<Action_at, Double>> rewardMap = new HashMap<State, HashMap<Action_at, Double>>();
		for (State state : states_map.values()) {
			Map<Action_at, Double> rewardActionMap = new HashMap<Action_at, Integer>();
			for (Action_at action : actionsMap.values()) {
				double reward = 0 ;
				if(action.isAccept_task()){
						double reward += state.getAvailable_task().reward - costPerKm * state.getAvailable_task().pathLength();
				}
				else {
					double reward = - costPerKm * action.getNext_city().distanceTo(state.getCurrentCity());
				}
				rewardActionMap.put(action, reward);
			}
			rewardMap.put(state, rewardActionMap);
		}

		//filling Transition Probability table
		Map<State, HashMap<State, HashMap<Action_at, Double>>> transitionMap = new HashMap<State, HashMap<State, <HashMap< Action_at, Double>>>();
		for (State statePrime : states_map.values()) {
			Map<State, HashMap<Action_at, Double>> transitionStateMap = new HashMap<State, HashMap<Action_at, Double>>();
			for (State state : states_map.values()) {
				Map<Action_at, Double> transitionStateActionMap = new HashMap<Action_at, Double>();
				for (Action_at action : actinsMap.values()) {
					Double transitionProbability = td.probability(action.getNext_city(), statePrime.getAvailable_task().deliveryCity);
					transitionStateActionMap.put(action, transitionProbability);
				}
				transitionStateMap.put(state, transitionStateActionMap);
			}
			transitionMap.put(statePrime, transitionStateMap);
		}


		//Offline training algorithm
		Map<State, Double> bestOfStateMap = new HashMap<State, Double>();


	@Override
	public Action act(Vehicle vehicle, Task availableTask) {
		Action action;

		if (availableTask == null || random.nextDouble() > pPickup) {
			City currentCity = vehicle.getCurrentCity();
			action = new Move(currentCity.randomNeighbor(random));
		} else {
			action = new Pickup(availableTask);
		}

		if (numActions >= 1) {
			System.out.println("The total profit after "+numActions+" actions is "+myAgent.getTotalProfit()+" (average profit: "+(myAgent.getTotalProfit() / (double)numActions)+")");
		}
		numActions++;

		return action;
	}


}
